import org.apache.spark.sql.{Dataset, Row, SparkSession}
import org.apache.spark.sql.functions._
import java.util.Properties

object SparkConsumerStream {
  def main(args: Array[String]): Unit = {
    // 1. Initialisation Spark
    val spark = SparkSession.builder()
      .appName("Kafka CSV Consumer Spark")
      .master("local[*]") // Ignor√© en cluster
      .config("spark.kafka.bootstrap.servers", "kafka:9092")
      .getOrCreate()
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // 2. Inf√©rence du sch√©ma √† partir d'un sample local
    val sampleDF = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .option("delimiter", ",")
      .csv("bank.csv")            // <-- adapte le chemin si besoin
    val schema = sampleDF.schema
    println("üìã Sch√©ma devin√© automatiquement :")
    schema.printTreeString()
    
    // 3. Lecture du flux Kafka
    val kafkaStream = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "kafka:9092")
      .option("subscribe", "csv-topic")
      .option("startingOffsets", "earliest")
      .option("failOnDataLoss", "false")
      .load()

    // 4. Parsing JSON ‚Üí colonnes structur√©es
    val parsedStream = kafkaStream
      .selectExpr("CAST(value AS STRING)")
      .as[String]
      .select(from_json($"value", schema).as("data"))
      .select("data.*")

    // 5. Pr√©paration de la connexion PostgreSQL
    val jdbcUrl = "jdbc:postgresql://postgres:5432/sparkdb"
    val pgProps = new Properties()
    pgProps.setProperty("user", "admin")
    pgProps.setProperty("password", "admin")
    pgProps.setProperty("driver", "org.postgresql.Driver")

    // 6. Traitement et √©criture par micro-batch
    val query = parsedStream.writeStream
      .foreachBatch { (batchDF: Dataset[Row], batchId: Long) =>
        println(s"‚ñ∂Ô∏è  Micro-batch $batchId")

        // üìå Sur le premier batch : overwrite, sinon append
        val writeMode = if (batchId == 0) "overwrite" else "append"

        // 6.1. Stockage des donn√©es brutes
        batchDF.write
          .mode(writeMode)
          .jdbc(jdbcUrl, "bank_data", pgProps)

        // 6.2. Filtrer uniquement les ¬´ yes ¬ª
        val dfYes = batchDF.filter($"deposit" === "yes")

        // ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
        // AGR√âGAT #1 : nombre de d√©p√¥ts ‚Äúyes‚Äù par m√©tier
        val byJob = dfYes
          .groupBy($"job")
          .count()
          .withColumnRenamed("count", "nb_depots")
        byJob.write.mode("append")
          .jdbc(jdbcUrl, "deposit_by_job", pgProps)

        // AGR√âGAT #2 : nombre de d√©p√¥ts ‚Äúyes‚Äù par situation familiale
        val byMarital = dfYes
          .groupBy($"marital")
          .count()
          .withColumnRenamed("count", "nb_depots")
        byMarital.write.mode("append")
          .jdbc(jdbcUrl, "deposit_by_marital", pgProps)

        // AGR√âGAT #3 : nombre de d√©p√¥ts ‚Äúyes‚Äù par niveau d‚Äô√©ducation
        val byEducation = dfYes
          .groupBy($"education")
          .count()
          .withColumnRenamed("count", "nb_depots")
        byEducation.write.mode("append")
          .jdbc(jdbcUrl, "deposit_by_education", pgProps)

        // AGR√âGAT #4 : taux de souscription ‚Äúyes‚Äù par type de contact
        val contactStats = batchDF
          .groupBy($"contact")
          .agg(
            count("*").as("total"),
            sum(when($"deposit" === "yes", 1).otherwise(0)).as("yes_count")
          )
          .withColumn("yes_rate", $"yes_count" / $"total")
        contactStats.write.mode("append")
          .jdbc(jdbcUrl, "yes_rate_by_contact", pgProps)

        // AGR√âGAT #5 : distribution d√©p√¥ts (yes/no) par type de logement
        val housingDist = batchDF
          .groupBy($"housing", $"deposit")
          .count()
          .withColumnRenamed("count", "nb")
        housingDist.write.mode("append")
          .jdbc(jdbcUrl, "deposit_by_housing", pgProps)

        println(s"‚úÖ Micro-batch $batchId correctement trait√©")
      }
      .outputMode("append")  // on accumule les r√©sultats par batch
      .start()

    query.awaitTermination()
  }
}
